{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjQJ4IxIVbSC",
        "outputId": "12a45eed-9a47-4d04-c850-be1dedb443e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastparquet\n",
            "  Downloading fastparquet-2024.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: pandas>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from fastparquet) (2.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fastparquet) (1.26.4)\n",
            "Collecting cramjam>=2.3 (from fastparquet)\n",
            "  Downloading cramjam-2.8.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from fastparquet) (2024.6.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fastparquet) (24.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5.0->fastparquet) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5.0->fastparquet) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5.0->fastparquet) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->fastparquet) (1.16.0)\n",
            "Downloading fastparquet-2024.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cramjam-2.8.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: cramjam, fastparquet\n",
            "Successfully installed cramjam-2.8.3 fastparquet-2024.5.0\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:132: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1cAthveg1d3MjrKJtMKGzfX3eH8HJ-dQp\n",
            "To: /content/MedNLI_dataset.zip\n",
            "100% 681k/681k [00:00<00:00, 92.3MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:132: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1-RxLiAq8SBsKZECM7vno8eSazvZ-jMbG\n",
            "From (redirected): https://drive.google.com/uc?id=1-RxLiAq8SBsKZECM7vno8eSazvZ-jMbG&confirm=t&uuid=cd3a99d4-0b08-413a-a4be-3a4612dc3946\n",
            "To: /content/modeln.pth\n",
            "100% 515M/515M [00:07<00:00, 65.6MB/s]\n",
            "Archive:  MedNLI_dataset.zip\n",
            "   creating: MedNLI_dataset/\n",
            "  inflating: MedNLI_dataset/valid-00000-of-00001-cc552de6d1a6fa4b.parquet  \n",
            "  inflating: MedNLI_dataset/train-00000-of-00001-210cfe9263b99806.parquet  \n",
            "  inflating: MedNLI_dataset/test-00000-of-00001-47685aa42db61e77.parquet  \n",
            "Collecting medialpy\n",
            "  Downloading medialpy-0.0.4-py3-none-any.whl.metadata (2.6 kB)\n",
            "Downloading medialpy-0.0.4-py3-none-any.whl (51 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.4/51.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: medialpy\n",
            "Successfully installed medialpy-0.0.4\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (13 kB)\n",
            "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n"
          ]
        }
      ],
      "source": [
        "!pip install fastparquet\n",
        "!gdown --id '1cAthveg1d3MjrKJtMKGzfX3eH8HJ-dQp'\n",
        "!gdown --id '1-RxLiAq8SBsKZECM7vno8eSazvZ-jMbG'\n",
        "!unzip MedNLI_dataset.zip\n",
        "!pip install medialpy\n",
        "!pip install contractions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset,DataLoader, random_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import string\n",
        "import medialpy\n",
        "import contractions\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "ewoX7EtwVy0i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec39d0be-4568-45c1-8df8-74bda859b78b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = pd.read_parquet('MedNLI_dataset/test-00000-of-00001-47685aa42db61e77.parquet', engine='fastparquet')\n",
        "train_data = pd.read_parquet('MedNLI_dataset/train-00000-of-00001-210cfe9263b99806.parquet', engine='fastparquet')\n",
        "valid_data = pd.read_parquet('MedNLI_dataset/valid-00000-of-00001-cc552de6d1a6fa4b.parquet', engine='fastparquet')"
      ],
      "metadata": {
        "id": "-imlKF7tWueQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_pre_and_hyp(query):\n",
        "    start_pre = query.find(\"[PRE]\") + len(\"[PRE]\")\n",
        "    end_pre = query.find(\"[HYP]\")\n",
        "    start_hyp = query.find(\"[HYP]\") + len(\"[HYP]\")\n",
        "    end_hyp = query.find(\"OUTPUT:\")\n",
        "    premise = query[start_pre:end_pre].strip()\n",
        "    hypothesis = query[start_hyp:end_hyp].strip()\n",
        "\n",
        "    return premise,hypothesis"
      ],
      "metadata": {
        "id": "yqLOKmtxXdQt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, x_list, y_list):\n",
        "        self.samples = []\n",
        "        for x,y in zip(x_list,y_list):\n",
        "            #x_tensor = torch.tensor(x,dtype = torch.float32)\n",
        "            y_tensor = torch.tensor(y,dtype = torch.float32)\n",
        "            self.samples.append((x[0],x[1],y_tensor))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.samples[idx]"
      ],
      "metadata": {
        "id": "hI1Ih4gsmSdz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gsarti/biobert-nli\")\n",
        "bert_model = AutoModel.from_pretrained(\"gsarti/biobert-nli\").to(device)"
      ],
      "metadata": {
        "id": "pFLPQCtOZMsx"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lists(data):\n",
        "    x_list = []\n",
        "    y_list = []\n",
        "    i =0\n",
        "    for query,answer in zip(data['query'],data['answer']):\n",
        "        i = i + 1\n",
        "        if answer == 'entailment':\n",
        "           y = [1,0,0]\n",
        "        elif answer == 'neutral':\n",
        "           y = [0,1,0]\n",
        "        elif answer == 'contradiction':\n",
        "           y = [0,0,1]\n",
        "        else:\n",
        "           print('should not get here')\n",
        "\n",
        "        premise,hypothesis = find_pre_and_hyp(query)\n",
        "        x_list.append((premise,hypothesis))\n",
        "        y_list.append(y)\n",
        "    return x_list,y_list"
      ],
      "metadata": {
        "id": "p_uPnftojy9W"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x_list,train_y_list = get_lists(train_data)\n",
        "test_x_list,test_y_list = get_lists(test_data)\n",
        "val_x_list,val_y_list = get_lists(valid_data)"
      ],
      "metadata": {
        "id": "6SMNxLIqbRor"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_x_list))\n",
        "print(len(train_y_list))\n",
        "print(len(test_x_list))\n",
        "print(len(test_y_list))\n",
        "print(len(val_x_list))\n",
        "print(len(val_y_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNGyCba4gR6O",
        "outputId": "7e0096aa-75aa-4f70-bac7-5af32ebef39f"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11232\n",
            "11232\n",
            "1422\n",
            "1422\n",
            "1395\n",
            "1395\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CustomDataset(train_x_list,train_y_list)\n",
        "test_dataset = CustomDataset(test_x_list,test_y_list)\n",
        "val_dataset = CustomDataset(val_x_list,val_y_list)"
      ],
      "metadata": {
        "id": "liSPv4JZ9VzM"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class embedding_layer(nn.Module):\n",
        "  def __init__(self,bert_model,tokenizer):\n",
        "    super(embedding_layer, self).__init__()\n",
        "    self.bert_model = bert_model\n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "  def forward(self, x):\n",
        "    with torch.no_grad():\n",
        "         tokens = tokenizer(x,return_tensors=\"pt\",padding=True).to(device)\n",
        "         we = bert_model(**tokens)['last_hidden_state'].to(device)\n",
        "    return we"
      ],
      "metadata": {
        "id": "e34Znrcrh78S"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,embedding_size):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.embedding_size = embedding_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        pe = torch.zeros(x.size(0), x.size(1), self.embedding_size).to(device)\n",
        "        div_term = torch.zeros(x.size(0), 1, self.embedding_size).to(device)\n",
        "        ks = torch.arange(self.embedding_size).float().to(device)\n",
        "        values = torch.exp(-torch.log(torch.tensor(1000.0)) * 2 * ks / self.embedding_size).to(device)\n",
        "        values = values.view(1, 1, -1).to(device)\n",
        "        div_term = div_term + values\n",
        "        x = x.reshape([x.shape[0],x.shape[1],1]).to(device)\n",
        "        pe[:, :, ::2] = torch.sin(x * div_term)[:, :, ::2].to(device)\n",
        "        pe[:, :, 1::2] = torch.cos(x * div_term)[:, :, 1::2].to(device)\n",
        "        pe = self.dropout(pe)\n",
        "        return pe"
      ],
      "metadata": {
        "id": "hZFjQji5jOiR"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embedding_size, heads):\n",
        "        super().__init__()\n",
        "        self.embedding_size = embedding_size\n",
        "        self.heads = heads\n",
        "        self.head_dim = embedding_size // heads\n",
        "        assert(self.heads * self.head_dim == self.embedding_size), \"Invalid number of heads\"\n",
        "        self.fc_values = nn.Linear(self.head_dim, self.head_dim, bias=False).to(device)\n",
        "        self.fc_keys = nn.Linear(self.head_dim, self.head_dim, bias=False).to(device)\n",
        "        self.fc_queries = nn.Linear(self.head_dim, self.head_dim, bias=False).to(device)\n",
        "        self.fc_out = nn.Linear(heads * self.head_dim, embedding_size).to(device)\n",
        "\n",
        "    def forward(self, values, keys, queries):\n",
        "        N = queries.shape[0]\n",
        "        value_len, key_len, query_len = values.shape[1], keys.shape[1], queries.shape[1]\n",
        "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
        "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
        "        queries = queries.reshape(N, query_len, self.heads, self.head_dim)\n",
        "        values = self.fc_values(values).to(device)\n",
        "        keys = self.fc_keys(keys).to(device)\n",
        "        queries = self.fc_queries(queries).to(device)\n",
        "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys]).to(device)\n",
        "        energy = torch.softmax(energy / (self.embedding_size ** 0.5), dim=3).to(device)\n",
        "        attention = torch.einsum(\"nhql,nlhd->nqhd\", [energy, values]).to(device)\n",
        "        attention = attention.reshape(N, query_len, self.heads * self.head_dim).to(device)\n",
        "        x = self.fc_out(attention)\n",
        "        return x"
      ],
      "metadata": {
        "id": "kWUt0eOEjSE3"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embedding_size, heads, forward_expansion, p):\n",
        "        super().__init__()\n",
        "        self.attention = MultiHeadAttention(embedding_size, heads)\n",
        "        self.norm1 = nn.LayerNorm(embedding_size)\n",
        "        self.feed_forward = nn.Sequential(nn.Linear(embedding_size, forward_expansion * embedding_size),\n",
        "                                          nn.ReLU(),\n",
        "                                          nn.Linear(forward_expansion * embedding_size, embedding_size))\n",
        "        self.norm2 = nn.LayerNorm(embedding_size)\n",
        "        self.dropout = nn.Dropout(p)\n",
        "    def forward(self, values, keys, queries):\n",
        "        attention_out = self.attention(values, keys, queries)\n",
        "        x = self.norm1(attention_out + queries)\n",
        "        x = self.dropout(x)\n",
        "        x = self.norm2(self.feed_forward(x) + x)\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "BARIDW8JjTYg"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, embedding_size=768,\n",
        "                 num_layers=1, forward_expansion=8, heads=8, p=0.1, device = device):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.word_embedding = embedding_layer(bert_model,tokenizer)\n",
        "        self.positional_embedding = PositionalEncoding(embedding_size)\n",
        "        self.layers = nn.ModuleList([TransformerBlock(embedding_size, heads, forward_expansion, p) for _ in range(num_layers)])\n",
        "        self.dropout = nn.Dropout(p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        we = self.word_embedding(x)\n",
        "        positions = torch.arange(0, we.size(1)).expand(we.size(0), we.size(1)).to(self.device)\n",
        "        pe = self.positional_embedding(positions)\n",
        "        x = self.dropout(we + pe)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, x, x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "QGiJnxVqjWao"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, embedding_size=768,\n",
        "                 num_layers=1, forward_expansion=8, heads=8, p=0.1):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(embedding_size, num_layers, heads,\n",
        "                               forward_expansion, p, device)\n",
        "\n",
        "    def forward(self, src):\n",
        "        enc_out = self.encoder(src).to(device)\n",
        "        return enc_out"
      ],
      "metadata": {
        "id": "bURW1ynJjZst"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NN, self).__init__()\n",
        "        self.dropout1 = nn.Dropout(0.1)\n",
        "        self.dropout2 = nn.Dropout(0.1)\n",
        "        self.transformer1 = Transformer().to(device)\n",
        "        self.transformer2 = Transformer().to(device)\n",
        "        self.fc1 = nn.Linear(768, 256).to(device)\n",
        "        self.fc2 = nn.Linear(256, 3).to(device)\n",
        "\n",
        "    def deabbreviation(self,text):\n",
        "        try:\n",
        "           return medialpy.find(text).meaning[0]\n",
        "        except:\n",
        "           return text\n",
        "\n",
        "    def preprocess_text(self,text):\n",
        "        text = contractions.fix(text)\n",
        "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "        tokens = word_tokenize(text)\n",
        "        ntokens = []\n",
        "        for token in tokens:\n",
        "            ntokens.append(self.deabbreviation(token).lower())\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        stop_words.remove('no')\n",
        "        stop_words.remove('not')\n",
        "        filtered_tokens = [word for word in ntokens if word not in stop_words]\n",
        "        processed_text = ' '.join(filtered_tokens)\n",
        "        return processed_text\n",
        "\n",
        "    def forward(self, x_pre,x_hyp):\n",
        "        nx_pre = ()\n",
        "        nx_hyp = ()\n",
        "        for t in x_pre:\n",
        "            nx_pre += (self.preprocess_text(t),)\n",
        "        for t in x_hyp:\n",
        "            nx_hyp += (self.preprocess_text(t),)\n",
        "        x_pre = nx_pre\n",
        "        x_hyp = nx_hyp\n",
        "        enc_x_pre = self.transformer1(x_pre)\n",
        "        enc_x_hyp = self.transformer2(x_hyp)\n",
        "        enc_x_pre = torch.mean(enc_x_pre,1)\n",
        "        enc_x_hyp = torch.mean(enc_x_hyp,1)\n",
        "        x = enc_x_pre * enc_x_hyp\n",
        "        x = F.relu(self.fc1(self.dropout1(x)))\n",
        "        x = nn.functional.softmax(self.fc2(self.dropout2(x)),dim=1)\n",
        "        return x"
      ],
      "metadata": {
        "id": "tA_HiJk8BLbm"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bsize = 32\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=bsize, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=bsize, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=bsize, shuffle=False)"
      ],
      "metadata": {
        "id": "ivNVXezMan8q"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model = NN()\n",
        "model = torch.load('modeln.pth')\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.00001)"
      ],
      "metadata": {
        "id": "-mBKC6VQBjvL"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_acc(model,data_loader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "    for x_pre, x_hyp, y in data_loader:\n",
        "        outputs = model(x_pre,x_hyp)\n",
        "        predicted_labels = torch.argmax(outputs, dim=1).cpu().numpy()\n",
        "        predictions.extend(predicted_labels)\n",
        "        true_label = torch.argmax(y, dim=1).cpu().numpy()\n",
        "        true_labels.extend(true_label)\n",
        "    model.train()\n",
        "    return accuracy_score(true_labels, predictions)"
      ],
      "metadata": {
        "id": "Qb8WFew945C2"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "     print(get_model_acc(model,test_loader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxX2hAisB3Rx",
        "outputId": "38a3b00f-a7bd-40c3-a935-36ca949ca2f8"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7538677918424754\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "num_epochs = 2\n",
        "for epoch in range(num_epochs):\n",
        "    for x_pre,x_hyp,y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(x_pre,x_hyp)\n",
        "        y = y.to(device)\n",
        "        loss = criterion(outputs, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    #print(get_model_acc(model,train_loader))\n",
        "    with torch.no_grad():\n",
        "         acc = get_model_acc(model,test_loader)\n",
        "    print(acc)\n",
        "    if acc >0.7539:\n",
        "       torch.save(model,'modelc.pth')\n",
        "       source_file_path = \"modelc.pth\"\n",
        "       destination_folder = \"/content/drive/My Drive/modelc.pth\"\n",
        "       shutil.copy(source_file_path, destination_folder)\n",
        "    print(loss.item())\n",
        "    print('===========================================================================')\n",
        "print('Training completed.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pz3oqSaVBYqg",
        "outputId": "09f11151-2505-4223-cc88-06f42120503c"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.749648382559775\n",
            "0.5517847537994385\n",
            "===========================================================================\n",
            "0.7524613220815752\n",
            "0.6138211488723755\n",
            "===========================================================================\n",
            "Training completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_model_acc(model,test_loader)"
      ],
      "metadata": {
        "id": "V1SA0k40CiWq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4f64c9f-e5bf-440e-8ae3-199b22eb9c52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7327707454289732"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_model_acc(model,val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VG-U1YDt8RpF",
        "outputId": "69f40a85-d3b4-4a46-c03a-41b08193b542"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7577060931899642"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_model_acc(model,train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orQfAOgo8XEY",
        "outputId": "e754ff95-3657-4973-9d95-ac784110c324"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8955662393162394"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    }
  ]
}