{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjQJ4IxIVbSC",
        "outputId": "04eb6f7e-626d-4c6b-9075-85aec0dff4c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.13.1-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (71.0.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.26.4)\n",
            "Using cached pybind11-2.13.1-py3-none-any.whl (238 kB)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.3-cp310-cp310-linux_x86_64.whl size=4246764 sha256=3e5a50d3ba8793191b2631e8adb0da5f0ba96b89329df57671f24f65289d40e5\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/a2/00/81db54d3e6a8199b829d58e02cec2ddb20ce3e59fad8d3c92a\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.3 pybind11-2.13.1\n",
            "Collecting fastparquet\n",
            "  Downloading fastparquet-2024.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: pandas>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from fastparquet) (2.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fastparquet) (1.26.4)\n",
            "Collecting cramjam>=2.3 (from fastparquet)\n",
            "  Downloading cramjam-2.8.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from fastparquet) (2024.6.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fastparquet) (24.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5.0->fastparquet) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5.0->fastparquet) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5.0->fastparquet) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->fastparquet) (1.16.0)\n",
            "Downloading fastparquet-2024.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cramjam-2.8.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: cramjam, fastparquet\n",
            "Successfully installed cramjam-2.8.3 fastparquet-2024.5.0\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:132: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1cAthveg1d3MjrKJtMKGzfX3eH8HJ-dQp\n",
            "To: /content/MedNLI_dataset.zip\n",
            "100% 681k/681k [00:00<00:00, 138MB/s]\n",
            "Archive:  MedNLI_dataset.zip\n",
            "   creating: MedNLI_dataset/\n",
            "  inflating: MedNLI_dataset/valid-00000-of-00001-cc552de6d1a6fa4b.parquet  \n",
            "  inflating: MedNLI_dataset/train-00000-of-00001-210cfe9263b99806.parquet  \n",
            "  inflating: MedNLI_dataset/test-00000-of-00001-47685aa42db61e77.parquet  \n",
            "Collecting medialpy\n",
            "  Downloading medialpy-0.0.4-py3-none-any.whl.metadata (2.6 kB)\n",
            "Downloading medialpy-0.0.4-py3-none-any.whl (51 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.4/51.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: medialpy\n",
            "Successfully installed medialpy-0.0.4\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (13 kB)\n",
            "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n"
          ]
        }
      ],
      "source": [
        "!pip install fasttext\n",
        "!pip install fastparquet\n",
        "!gdown --id '1cAthveg1d3MjrKJtMKGzfX3eH8HJ-dQp'\n",
        "!unzip MedNLI_dataset.zip\n",
        "!pip install medialpy\n",
        "!pip install contractions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title imports\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset,DataLoader, random_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import time\n",
        "import string\n",
        "import medialpy\n",
        "import contractions\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import time\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "ewoX7EtwVy0i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c5bbb1d-d091-48ff-93f4-c9ae5456256c",
        "cellView": "form"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Dataset functions\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, x_list, y_list):\n",
        "        self.samples = []\n",
        "        for x,y in zip(x_list,y_list):\n",
        "            #x_tensor = torch.tensor(x,dtype = torch.float32)\n",
        "            y_tensor = torch.tensor(y,dtype = torch.float32)\n",
        "            self.samples.append((x[0],x[1],y_tensor))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.samples[idx]\n",
        "\n",
        "def find_pre_and_hyp(query):\n",
        "    start_pre = query.find(\"[PRE]\") + len(\"[PRE]\")\n",
        "    end_pre = query.find(\"[HYP]\")\n",
        "    start_hyp = query.find(\"[HYP]\") + len(\"[HYP]\")\n",
        "    end_hyp = query.find(\"OUTPUT:\")\n",
        "    premise = query[start_pre:end_pre].strip()\n",
        "    hypothesis = query[start_hyp:end_hyp].strip()\n",
        "\n",
        "    return premise,hypothesis\n",
        "\n",
        "def get_lists(data):\n",
        "    x_list = []\n",
        "    y_list = []\n",
        "    i =0\n",
        "    for query,answer in zip(data['query'],data['answer']):\n",
        "        i = i + 1\n",
        "        if answer == 'entailment':\n",
        "           y = [1,0,0]\n",
        "        elif answer == 'neutral':\n",
        "           y = [0,1,0]\n",
        "        elif answer == 'contradiction':\n",
        "           y = [0,0,1]\n",
        "        else:\n",
        "           print('should not get here')\n",
        "\n",
        "        premise,hypothesis = find_pre_and_hyp(query)\n",
        "        x_list.append((premise,hypothesis))\n",
        "        y_list.append(y)\n",
        "    return x_list,y_list"
      ],
      "metadata": {
        "cellView": "form",
        "id": "lZTFx8njFKgk"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Encoder functions\n",
        "class embedding_layer(nn.Module):\n",
        "  def __init__(self,bert_model,tokenizer):\n",
        "    super(embedding_layer, self).__init__()\n",
        "    self.bert_model = bert_model\n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "  def forward(self, x):\n",
        "    with torch.no_grad():\n",
        "         s = tokenizer(x,return_tensors=\"pt\",padding=True).to(device)\n",
        "         vec = bert_model(**s)['last_hidden_state'].to(device)\n",
        "    return vec\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, seq_len,embedding_size):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.embedding_size = embedding_size\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def forward(self, x):\n",
        "        pe = torch.zeros(x.size(0), x.size(1), self.embedding_size).to(device)\n",
        "        div_term = torch.zeros(x.size(0), 1, self.embedding_size).to(device)\n",
        "        ks = torch.arange(self.embedding_size).float().to(device)\n",
        "        values = torch.exp(-torch.log(torch.tensor(1000.0)) * 2 * ks / self.embedding_size).to(device)\n",
        "        values = values.view(1, 1, -1).to(device)\n",
        "        div_term = div_term + values\n",
        "        x = x.reshape([x.shape[0],x.shape[1],1]).to(device)\n",
        "        pe[:, :, ::2] = torch.sin(x * div_term)[:, :, ::2].to(device)\n",
        "        pe[:, :, 1::2] = torch.cos(x * div_term)[:, :, 1::2].to(device)\n",
        "        return self.dropout(pe)\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embedding_size, heads):\n",
        "        super().__init__()\n",
        "        self.embedding_size = embedding_size\n",
        "        self.heads = heads\n",
        "        self.head_dim = embedding_size // heads\n",
        "        assert(self.heads * self.head_dim == self.embedding_size), \"Invalid number of heads\"\n",
        "        self.fc_values = nn.Linear(self.head_dim, self.head_dim, bias=False).to(device)\n",
        "        self.fc_keys = nn.Linear(self.head_dim, self.head_dim, bias=False).to(device)\n",
        "        self.fc_queries = nn.Linear(self.head_dim, self.head_dim, bias=False).to(device)\n",
        "        self.fc_out = nn.Linear(heads * self.head_dim, embedding_size).to(device)\n",
        "\n",
        "    def forward(self, values, keys, queries, mask):\n",
        "        N = queries.shape[0]\n",
        "        value_len, key_len, query_len = values.shape[1], keys.shape[1], queries.shape[1]\n",
        "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
        "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
        "        queries = queries.reshape(N, query_len, self.heads, self.head_dim)\n",
        "        values = self.fc_values(values).to(device)\n",
        "        keys = self.fc_keys(keys).to(device)\n",
        "        queries = self.fc_queries(queries).to(device)\n",
        "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys]).to(device)\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask.to(device) == 0, float(\"-1e20\")).to(device)\n",
        "        energy = torch.softmax(energy / (self.embedding_size ** 0.5), dim=3).to(device)\n",
        "        attention = torch.einsum(\"nhql,nlhd->nqhd\", [energy, values]).to(device)\n",
        "        attention = attention.reshape(N, query_len, self.heads * self.head_dim).to(device)\n",
        "        out = self.fc_out(attention)\n",
        "        return out\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embedding_size, heads, forward_expansion, p):\n",
        "        super().__init__()\n",
        "        self.attention = MultiHeadAttention(embedding_size, heads)\n",
        "        self.norm1 = nn.LayerNorm(embedding_size)\n",
        "        self.feed_forward = nn.Sequential(nn.Linear(embedding_size, forward_expansion * embedding_size),\n",
        "                                          nn.ReLU(),\n",
        "                                          nn.Linear(forward_expansion * embedding_size, embedding_size))\n",
        "        self.norm2 = nn.LayerNorm(embedding_size)\n",
        "        self.dropout = nn.Dropout(p)\n",
        "    def forward(self, values, keys, queries, mask):\n",
        "        attention_out = self.attention(values, keys, queries, mask)\n",
        "        x = self.norm1(attention_out + queries)\n",
        "        x = self.dropout(x)\n",
        "        ff_out = self.feed_forward(x)\n",
        "        out = self.norm2(ff_out + x)\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, src_vocab_size, embedding_size, num_layers, heads,\n",
        "                 forward_expansion, max_length, p, device):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.word_embedding = embedding_layer(bert_model,tokenizer)\n",
        "        self.positional_embedding = PositionalEncoding(max_length, embedding_size)\n",
        "        self.layers = nn.ModuleList([TransformerBlock(embedding_size, heads, forward_expansion, p) for _ in range(num_layers)])\n",
        "        self.dropout = nn.Dropout(p)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        mask = None\n",
        "        pe = self.word_embedding(x)\n",
        "        N = pe.size(0)\n",
        "        seq_len = pe.size(1)\n",
        "        positions = torch.arange(0, seq_len).expand(N, seq_len).to(self.device)\n",
        "        out = self.dropout((pe + self.positional_embedding(positions)))\n",
        "        for layer in self.layers:\n",
        "            out = layer(out, out, out ,mask)\n",
        "        return out\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, src_pad_idx, embedding_size=768,\n",
        "                 num_layers=1, forward_expansion=8, heads=8, max_length=100, p=0.1):\n",
        "        super().__init__()\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.encoder = Encoder(src_vocab_size, embedding_size, num_layers, heads,\n",
        "                               forward_expansion, max_length, p, device)\n",
        "\n",
        "    def get_src_mask(self, src):\n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2).to(device)\n",
        "        return src_mask\n",
        "\n",
        "    def forward(self, src):\n",
        "        src_mask = None\n",
        "        enc_out = self.encoder(src, src_mask).to(device)\n",
        "        return enc_out\n",
        "\n",
        "class NN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NN, self).__init__()\n",
        "        self.dropout1 = nn.Dropout(0.1)\n",
        "        self.dropout2 = nn.Dropout(0.1)\n",
        "        self.transformer1 = Transformer(src_vocab_size, src_pad_idx).to(device)\n",
        "        self.transformer2 = Transformer(src_vocab_size, src_pad_idx).to(device)\n",
        "        self.fc1 = nn.Linear(768, 256).to(device)\n",
        "        self.fc2 = nn.Linear(256, 3).to(device)\n",
        "\n",
        "    def deabbreviation(self,text):\n",
        "        try:\n",
        "           return medialpy.find(text).meaning[0]\n",
        "        except:\n",
        "           return text\n",
        "\n",
        "    def preprocess_text(self,text):\n",
        "        text = contractions.fix(text)\n",
        "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "        tokens = word_tokenize(text)\n",
        "        ntokens = []\n",
        "        for token in tokens:\n",
        "            ntokens.append(self.deabbreviation(token).lower())\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        stop_words.remove('no')\n",
        "        stop_words.remove('not')\n",
        "        filtered_tokens = [word for word in ntokens if word not in stop_words]\n",
        "        processed_text = ' '.join(filtered_tokens)\n",
        "        return processed_text\n",
        "\n",
        "    def forward(self, x_pre,x_hyp):\n",
        "        nx_pre = ()\n",
        "        nx_hyp = ()\n",
        "        for t in x_pre:\n",
        "            nx_pre += (self.preprocess_text(t),)\n",
        "        for t in x_hyp:\n",
        "            nx_hyp += (self.preprocess_text(t),)\n",
        "        x_pre = nx_pre\n",
        "        x_hyp = nx_hyp\n",
        "        enc_x_pre = self.transformer1(x_pre)\n",
        "        enc_x_hyp = self.transformer2(x_hyp)\n",
        "        enc_x_pre = torch.mean(enc_x_pre,1)\n",
        "        enc_x_hyp = torch.mean(enc_x_hyp,1)\n",
        "        x = enc_x_pre*enc_x_hyp\n",
        "        x = F.relu(self.fc1(self.dropout1(x)))\n",
        "        x = nn.functional.softmax(self.fc2(self.dropout2(x)),dim=1)\n",
        "        return x\n",
        "\n",
        "class NNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NNN, self).__init__()\n",
        "        self.dropout1 = nn.Dropout(0.1)\n",
        "        self.dropout2 = nn.Dropout(0.1)\n",
        "        self.transformer1 = Transformer(src_vocab_size, src_pad_idx).to(device)\n",
        "        self.transformer2 = Transformer(src_vocab_size, src_pad_idx).to(device)\n",
        "        self.fc1 = nn.Linear(768*2, 256).to(device)\n",
        "        self.fc2 = nn.Linear(256, 2).to(device)\n",
        "\n",
        "    def deabbreviation(self,text):\n",
        "        try:\n",
        "           return medialpy.find(text).meaning[0]\n",
        "        except:\n",
        "           return text\n",
        "\n",
        "    def preprocess_text(self,text):\n",
        "        text = contractions.fix(text)\n",
        "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "        tokens = word_tokenize(text)\n",
        "        ntokens = []\n",
        "        for token in tokens:\n",
        "            ntokens.append(self.deabbreviation(token).lower())\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        stop_words.remove('no')\n",
        "        stop_words.remove('not')\n",
        "        filtered_tokens = [word for word in ntokens if word not in stop_words]\n",
        "        processed_text = ' '.join(filtered_tokens)\n",
        "        return processed_text\n",
        "\n",
        "    def forward(self, x_pre,x_hyp):\n",
        "        nx_pre = ()\n",
        "        nx_hyp = ()\n",
        "        for t in x_pre:\n",
        "            nx_pre += (self.preprocess_text(t),)\n",
        "        for t in x_hyp:\n",
        "            nx_hyp += (self.preprocess_text(t),)\n",
        "        x_pre = nx_pre\n",
        "        x_hyp = nx_hyp\n",
        "        enc_x_pre = self.transformer1(x_pre)\n",
        "        enc_x_hyp = self.transformer2(x_hyp)\n",
        "        enc_x_pre = torch.mean(enc_x_pre,1)\n",
        "        enc_x_hyp = torch.mean(enc_x_hyp,1)\n",
        "        x = torch.cat((enc_x_pre,enc_x_hyp),1)\n",
        "        x = F.relu(self.fc1(self.dropout1(x)))\n",
        "        x = nn.functional.softmax(self.fc2(self.dropout2(x)),dim=1)\n",
        "        return x\n",
        "\n",
        "def get_model_acc(model,data_loader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "    for x_pre, x_hyp, y in data_loader:\n",
        "        outputs = model(x_pre,x_hyp)\n",
        "        predicted_labels = torch.argmax(outputs, dim=1).cpu().numpy()\n",
        "        predictions.extend(predicted_labels)\n",
        "        l1 = torch.argmax(y, dim=1).cpu().numpy()\n",
        "        true_labels.extend(l1)\n",
        "    model.train()\n",
        "    return accuracy_score(true_labels, predictions)\n",
        "\n",
        "def inference(pre,hyp):\n",
        "    return model((pre,),(hyp,))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "IN3DhyIkFg0D"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title prepare for training main model\n",
        "test_data = pd.read_parquet('MedNLI_dataset/test-00000-of-00001-47685aa42db61e77.parquet', engine='fastparquet')\n",
        "train_data = pd.read_parquet('MedNLI_dataset/train-00000-of-00001-210cfe9263b99806.parquet', engine='fastparquet')\n",
        "valid_data = pd.read_parquet('MedNLI_dataset/valid-00000-of-00001-cc552de6d1a6fa4b.parquet', engine='fastparquet')\n",
        "###################################################################################################################\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gsarti/biobert-nli\")\n",
        "bert_model = AutoModel.from_pretrained(\"gsarti/biobert-nli\").to(device)\n",
        "src_pad_idx = 0\n",
        "src_vocab_size = bert_model.config.vocab_size\n",
        "####################################################################################################################\n",
        "train_x_list,train_y_list = get_lists(train_data)\n",
        "test_x_list,test_y_list = get_lists(test_data)\n",
        "val_x_list,val_y_list = get_lists(valid_data)\n",
        "####################################################################################################################\n",
        "train_dataset = CustomDataset(train_x_list,train_y_list)\n",
        "test_dataset = CustomDataset(test_x_list,test_y_list)\n",
        "val_dataset = CustomDataset(val_x_list,val_y_list)\n",
        "####################################################################################################################\n",
        "bsize = 32\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=bsize, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=bsize, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=bsize, shuffle=False)\n",
        "#####################################################################################################################\n",
        "model = NN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.0001)\n",
        "#####################################################################################################################"
      ],
      "metadata": {
        "cellView": "form",
        "id": "dwMmzOZYGmuW"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    for x_pre,x_hyp,y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(x_pre,x_hyp)\n",
        "        y = y.to(device)\n",
        "        loss = criterion(outputs, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    #print(get_model_acc(model,train_loader))\n",
        "    print(get_model_acc(model,test_loader))\n",
        "    print(loss.item())\n",
        "    time.sleep(20)\n",
        "    print('===========================================================================')\n",
        "print('Training completed.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pz3oqSaVBYqg",
        "outputId": "5ac11470-6957-435a-faac-fc3026ad4d9d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6026722925457103\n",
            "0.8691142797470093\n",
            "===========================================================================\n",
            "0.7011251758087201\n",
            "0.8080055713653564\n",
            "===========================================================================\n",
            "0.69901547116737\n",
            "0.8341015577316284\n",
            "===========================================================================\n",
            "0.7144866385372715\n",
            "0.727597177028656\n",
            "===========================================================================\n",
            "0.7165963431786216\n",
            "0.8032186627388\n",
            "===========================================================================\n",
            "0.7088607594936709\n",
            "1.0074548721313477\n",
            "===========================================================================\n",
            "0.7257383966244726\n",
            "0.7155419588088989\n",
            "===========================================================================\n",
            "0.7278481012658228\n",
            "0.7220139503479004\n",
            "===========================================================================\n",
            "0.7116736990154712\n",
            "0.6873470544815063\n",
            "===========================================================================\n",
            "0.720112517580872\n",
            "0.7042057514190674\n",
            "===========================================================================\n",
            "Training completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title reli model functions\n",
        "def get_trust_model_inf(trustmodel,data_loader):\n",
        "    trustmodel.eval()\n",
        "    model.eval()\n",
        "    trust_x_list = []\n",
        "    untrust_x_list = []\n",
        "    trust_y_list = []\n",
        "    untrust_y_list = []\n",
        "    for x_pre,x_hyp,y in data_loader:\n",
        "        outputs = model(x_pre,x_hyp)\n",
        "        outputs_tw = trustmodel(x_pre,x_hyp)\n",
        "        y = y.to(device)\n",
        "        for i in range(len(outputs)):\n",
        "                 p = get_inference_from_tns(outputs[i])\n",
        "                 t = get_inference_from_tns(y[i])\n",
        "                 tw = get_trustworthy_from_tns(outputs_tw[i])\n",
        "                 if tw == 'trustworthy':\n",
        "                    trust_x_list.append((x_pre[i],x_hyp[i]))\n",
        "                    trust_y_list.append(y[i].tolist())\n",
        "                 elif tw == 'untrustworthy':\n",
        "                    untrust_x_list.append((x_pre[i],x_hyp[i]))\n",
        "                    untrust_y_list.append(y[i].tolist())\n",
        "                 else:\n",
        "                    print('shouldnt get here')\n",
        "    trust_dataset = CustomDataset(trust_x_list,trust_y_list)\n",
        "    untrust_dataset = CustomDataset(untrust_x_list,untrust_y_list)\n",
        "    trust_data_loader = torch.utils.data.DataLoader(trust_dataset, batch_size=32, shuffle=True)\n",
        "    untrust_data_loader = torch.utils.data.DataLoader(untrust_dataset, batch_size=32, shuffle=True)\n",
        "    print('trust num: ' + str(len(trust_y_list)))\n",
        "    print('untrust num: ' + str(len(untrust_y_list)))\n",
        "    print('trust acc: ' + str(get_model_acc(model,trust_data_loader)))\n",
        "    print('untrust acc: ' + str(get_model_acc(model,untrust_data_loader)))\n",
        "    return trust_dataset,untrust_dataset\n",
        "\n",
        "def get_inference_from_tns(tns):\n",
        "    infr = np.argmax(tns.cpu().detach().numpy())\n",
        "    if infr == 0:\n",
        "       return 'entailment'\n",
        "    elif infr == 1:\n",
        "       return 'neutral'\n",
        "    elif infr == 2:\n",
        "       return 'contradiction'\n",
        "    else:\n",
        "       return 'shouldnt get here'\n",
        "\n",
        "def get_trustworthy_from_tns(tns):\n",
        "    infr = np.argmax(tns.cpu().detach().numpy())\n",
        "    if infr == 0:\n",
        "       return 'trustworthy'\n",
        "    elif infr == 1:\n",
        "       return 'untrustworthy'\n",
        "    else:\n",
        "       return 'shouldnt get here'"
      ],
      "metadata": {
        "cellView": "form",
        "id": "NelWOGvHIrFu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title prepare for training reli model\n",
        "correct_label = int(get_model_acc(model,train_loader)*11232)\n",
        "uncorrect_label = 11232 - correct_label\n",
        "c_visit = 0\n",
        "uc_visit = 0\n",
        "train_relix_list = []\n",
        "train_reliy_list = []\n",
        "for x_pre, x_hyp, y in train_loader:\n",
        "    outputs = model(x_pre,x_hyp)\n",
        "    predicted_labels = torch.argmax(outputs, dim=1).cpu().numpy()\n",
        "    true_labels = torch.argmax(y, dim=1).cpu().numpy()\n",
        "    for i in range(len(predicted_labels)):\n",
        "          predicted_label = predicted_labels[i]\n",
        "          true_label = true_labels[i]\n",
        "          if predicted_label != true_label:\n",
        "             if uc_visit <= min(correct_label,uncorrect_label):\n",
        "                uc_visit +=1\n",
        "                premise = x_pre[i]\n",
        "                hypothesis = x_hyp[i]\n",
        "                train_relix_list.append((premise,hypothesis))\n",
        "                train_reliy_list.append([0,1])\n",
        "          else:\n",
        "             if c_visit <= min(correct_label,uncorrect_label):\n",
        "                c_visit +=1\n",
        "                premise = x_pre[i]\n",
        "                hypothesis = x_hyp[i]\n",
        "                train_relix_list.append((premise,hypothesis))\n",
        "                train_reliy_list.append([1,0])\n",
        "relitrain_dataset = CustomDataset(train_relix_list,train_reliy_list)\n",
        "###########################################################################################\n",
        "val_relix_list = []\n",
        "val_reliy_list = []\n",
        "for x_pre, x_hyp, y in val_loader:\n",
        "    outputs = model(x_pre,x_hyp)\n",
        "    predicted_labels = torch.argmax(outputs, dim=1).cpu().numpy()\n",
        "    true_labels = torch.argmax(y, dim=1).cpu().numpy()\n",
        "    for i in range(len(predicted_labels)):\n",
        "          predicted_label = predicted_labels[i]\n",
        "          true_label = true_labels[i]\n",
        "          if predicted_label != true_label:\n",
        "             premise = x_pre[i]\n",
        "             hypothesis = x_hyp[i]\n",
        "             val_relix_list.append((premise,hypothesis))\n",
        "             val_reliy_list.append([0,1])\n",
        "          else:\n",
        "             premise = x_pre[i]\n",
        "             hypothesis = x_hyp[i]\n",
        "             val_relix_list.append((premise,hypothesis))\n",
        "             val_reliy_list.append([1,0])\n",
        "relival_dataset = CustomDataset(val_relix_list,val_reliy_list)\n",
        "###########################################################################################\n",
        "test_relix_list = []\n",
        "test_reliy_list = []\n",
        "for x_pre, x_hyp, y in test_loader:\n",
        "    outputs = model(x_pre,x_hyp)\n",
        "    predicted_labels = torch.argmax(outputs, dim=1).cpu().numpy()\n",
        "    true_labels = torch.argmax(y, dim=1).cpu().numpy()\n",
        "    for i in range(len(predicted_labels)):\n",
        "          predicted_label = predicted_labels[i]\n",
        "          true_label = true_labels[i]\n",
        "          if predicted_label != true_label:\n",
        "             premise = x_pre[i]\n",
        "             hypothesis = x_hyp[i]\n",
        "             test_relix_list.append((premise,hypothesis))\n",
        "             test_reliy_list.append([0,1])\n",
        "          else:\n",
        "             premise = x_pre[i]\n",
        "             hypothesis = x_hyp[i]\n",
        "             test_relix_list.append((premise,hypothesis))\n",
        "             test_reliy_list.append([1,0])\n",
        "relitest_dataset = CustomDataset(test_relix_list,test_reliy_list)\n",
        "###################################################################################################\n",
        "relitrain_loader = torch.utils.data.DataLoader(relitrain_dataset, batch_size=bsize, shuffle=True)\n",
        "relival_loader = torch.utils.data.DataLoader(relival_dataset, batch_size=bsize, shuffle=True)\n",
        "relitest_loader = torch.utils.data.DataLoader(relitest_dataset, batch_size=bsize, shuffle=True)\n",
        "###################################################################################################\n",
        "relimodel = NNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(relimodel.parameters(), lr=0.0001)"
      ],
      "metadata": {
        "id": "WPvNOoaLH3h-",
        "cellView": "form"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "relimodel.train()\n",
        "num_epochs = 3\n",
        "for epoch in range(num_epochs):\n",
        "    for x_pre,x_hyp,y in relitrain_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = relimodel(x_pre,x_hyp)\n",
        "        y = y.to(device)\n",
        "        loss = criterion(outputs, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    get_trust_model_inf(relimodel,test_loader)\n",
        "    print(loss.item())\n",
        "    print('==================================')\n",
        "print('Training completed.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjHSdvnVDQTT",
        "outputId": "f63d0226-b41c-47d0-9cdd-2519605c6519"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trust num: 489\n",
            "untrust num: 933\n",
            "trust acc: 0.8179959100204499\n",
            "untrust acc: 0.6495176848874598\n",
            "0.6973949670791626\n",
            "==================================\n",
            "trust num: 420\n",
            "untrust num: 1002\n",
            "trust acc: 0.8738095238095238\n",
            "untrust acc: 0.6447105788423154\n",
            "0.6374236941337585\n",
            "==================================\n",
            "trust num: 489\n",
            "untrust num: 933\n",
            "trust acc: 0.8425357873210634\n",
            "untrust acc: 0.639871382636656\n",
            "0.5671142339706421\n",
            "==================================\n",
            "Training completed.\n"
          ]
        }
      ]
    }
  ]
}